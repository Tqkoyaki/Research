{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ed8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Obtains the file and cleans it\n",
    "FILE_PATH = 'data/Advertising.csv' \n",
    "df = pd.read_csv(FILE_PATH).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Obtains our x and y vectors\n",
    "x = df.TV.to_numpy().reshape(-1, 1)\n",
    "y = df.Sales.to_numpy().reshape(-1, 1)\n",
    "\n",
    "x_tensor = Variable(torch.from_numpy(x).type(torch.FloatTensor)) # type: ignore\n",
    "y_tensor = Variable(torch.from_numpy(y).type(torch.FloatTensor)) # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d25301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b5f6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2028febd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StraightLine_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = nn.Parameter(torch.tensor(1.0))\n",
    "        self.b = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.m * x + self.b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ccd0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nLogLikelyhood_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = StraightLine_v1()\n",
    "        self.sigmaN = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        pred = self.f(x)\n",
    "        return 0.5 * (torch.log(self.sigmaN ** 2) + 0.5 * ((y - pred) ** 2) / (self.sigmaN ** 2)).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b485ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "\n",
    "class nLogLikelyhood_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = StraightLine_v1()\n",
    "        self.sigmaN = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        pred = self.f(x)\n",
    "        return (-1.) * Normal(pred, self.sigmaN).log_prob(y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1acec64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.exponential import Exponential\n",
    "\n",
    "class maxPosterior_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = StraightLine_v1()\n",
    "        self.sigmaN = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        pred = self.f(x)\n",
    "        nLogLik = (-1.) * Normal(pred, self.sigmaN).log_prob(y).sum()\n",
    "        nLogPriorM = (-1.) * Normal(0, 1).log_prob(self.f.m)\n",
    "        #nLogPriorB = (-1.) * Exponential(2.0).log_prob(self.f.b)\n",
    "        nLogPriorB = (-1.) * Normal(0, 10).log_prob(self.f.b)\n",
    "        \n",
    "        return nLogLik + nLogPriorM + nLogPriorB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07bd2a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StraightLine_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.muM = nn.Parameter(torch.tensor(1.0))\n",
    "        self.sigmaM = nn.Parameter(torch.tensor(1.0))\n",
    "        self.muB = nn.Parameter(torch.tensor(1.0))\n",
    "        self.sigmaB = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "        self.samples = 100\n",
    "        \n",
    "        #self.epsilonM = self.muM.data.new(self.muM.size()).normal_()\n",
    "        #self.epsilonB = self.muB.data.new(self.muB.size()).normal_()\n",
    "        \n",
    "        #self.m = self.muM + torch.exp(self.sigmaM) * self.epsilonM\n",
    "        #self.b = self.muB + torch.exp(self.sigmaB) * self.epsilonB\n",
    "        self.m = self.muM + torch.exp(self.sigmaM) * Normal(0, 1).sample(torch.Size([1, self.samples]))\n",
    "        self.b = self.muB + torch.exp(self.sigmaB) * Normal(0, 1).sample(torch.Size([1, self.samples]))\n",
    "        #self.m = self.muM + torch.exp(self.sigmaM) * Normal(0, 1).sample()\n",
    "        #self.b = self.muB + torch.exp(self.sigmaB) * Normal(0, 1).sample()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #return x * self.m + self.b\n",
    "        #return self.m * x + self.b\n",
    "        \n",
    "        self.m = self.muM + torch.exp(self.sigmaM) * Normal(0, 1).sample(torch.Size([1, self.samples]))\n",
    "        self.b = self.muB + torch.exp(self.sigmaB) * Normal(0, 1).sample(torch.Size([1, self.samples]))\n",
    "\n",
    "        return torch.matmul(x, self.m) + self.b.repeat(x.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c83bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class variationalBayes_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = StraightLine_v2()\n",
    "        self.sigmaN = nn.Parameter(torch.tensor(1.0))\n",
    "        self.prior_M_std = torch.tensor([1.])\n",
    "        self.prior_B_std = torch.tensor([10.])\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        pred = self.f(x)\n",
    "        \n",
    "        y_truth = y.reshape(y.shape[0], -1).repeat(1, self.f.samples)\n",
    "        nLogLik = (-1.) * Normal(pred, self.sigmaN).log_prob(y_truth).sum() / self.f.samples\n",
    "\n",
    "        nLogPriorM = torch.log(self.prior_M_std) + 0.5 * (self.f.muM / self.prior_M_std) ** 2 + 0.5 * (self.f.sigmaM / self.prior_M_std) ** 2\n",
    "        nLogPriorB = torch.log(self.prior_B_std) + 0.5 * (self.f.muB / self.prior_B_std) ** 2 + 0.5 * (self.f.sigmaB / self.prior_B_std) ** 2\n",
    "        \n",
    "        LogVarPostM = (-1.) * torch.log(self.f.sigmaM) \n",
    "        LogVarPostB = (-1.) * torch.log(self.f.sigmaB)\n",
    "        \n",
    "        return LogVarPostM + LogVarPostB + nLogLik + nLogPriorM + nLogPriorB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4226907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StraightLine_v3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.muM = nn.Parameter(torch.tensor(1.0))\n",
    "        self.sigmaM = nn.Parameter(torch.tensor(1.0))\n",
    "        self.muB = nn.Parameter(torch.tensor(1.0))\n",
    "        self.sigmaB = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        #self.m = m\n",
    "        #self.b = b\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        m_epsilons = Variable(self.muM.data.new(self.muM.size()).normal_())\n",
    "        b_epsilons = Variable(self.muB.data.new(self.muB.size()).normal_())\n",
    "        \n",
    "        m_stds = torch.exp(self.sigmaM)\n",
    "        b_stds = torch.exp(self.sigmaB)\n",
    "        \n",
    "        m_sample = self.muM + m_stds * m_epsilons\n",
    "        b_sample = self.muB + b_stds * b_epsilons\n",
    "        \n",
    "        return m_sample * x + b_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02c53563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class variationalBayes_v2(nn.Module):\n",
    "    def __init__(self, num_samples = 40):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        self.f = StraightLine_v3()\n",
    "        \n",
    "        self.sigmaN = nn.Parameter(torch.tensor(1.0))\n",
    "        self.prior_M_std = torch.tensor([1.])\n",
    "        self.prior_B_std = torch.tensor([10.])\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        nLogLik = 0.0\n",
    "        \n",
    "        for i in range(self.num_samples):\n",
    "            \n",
    "            pred = self.f(x)\n",
    "            #y_truth = y.reshape(y.shape[0], -1).repeat(1, self.f.samples)\n",
    "            nLogLik = nLogLik + (-1.) * Normal(pred, self.sigmaN).log_prob(y).sum() \n",
    "            \n",
    "        nLogLik = nLogLik / self.num_samples\n",
    " \n",
    "        nLogPriorM = torch.log(self.prior_M_std) + 0.5 * (self.f.muM / self.prior_M_std) ** 2 + 0.5 * (self.f.sigmaM / self.prior_M_std) ** 2\n",
    "        nLogPriorB = torch.log(self.prior_B_std) + 0.5 * (self.f.muB / self.prior_B_std) ** 2 + 0.5 * (self.f.sigmaB / self.prior_B_std) ** 2\n",
    "        \n",
    "        LogVarPostM = (-1.) * torch.log(self.f.sigmaM) \n",
    "        LogVarPostB = (-1.) * torch.log(self.f.sigmaB)\n",
    "        \n",
    "        return LogVarPostM + LogVarPostB + nLogLik + nLogPriorM + nLogPriorB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e00fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class variationalBayes_v3(nn.Module):\n",
    "    def __init__(self, num_samples = 40):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        self.f = StraightLine_v2()\n",
    "        \n",
    "        self.sigmaN = nn.Parameter(torch.tensor(1.0))\n",
    "        self.prior_M_std = torch.tensor([1.])\n",
    "        self.prior_B_std = torch.tensor([10.])\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        nLogLik = 0.0\n",
    "        \n",
    "        for i in range(self.num_samples):\n",
    "            \n",
    "            pred = self.f(x)\n",
    "            #y_truth = y.reshape(y.shape[0], -1).repeat(1, self.f.samples)\n",
    "            nLogLik = nLogLik + (-1.) * Normal(pred, self.sigmaN).log_prob(y).sum() \n",
    "            \n",
    "        nLogLik = nLogLik / self.num_samples\n",
    " \n",
    "        nLogPriorM = torch.log(self.prior_M_std) + 0.5 * (self.f.muM / self.prior_M_std) ** 2 + 0.5 * (self.f.sigmaM / self.prior_M_std) ** 2\n",
    "        nLogPriorB = torch.log(self.prior_B_std) + 0.5 * (self.f.muB / self.prior_B_std) ** 2 + 0.5 * (self.f.sigmaB / self.prior_B_std) ** 2\n",
    "        \n",
    "        LogVarPostM = (-1.) * torch.log(self.f.sigmaM) \n",
    "        LogVarPostB = (-1.) * torch.log(self.f.sigmaB)\n",
    "        \n",
    "        return LogVarPostM + LogVarPostB + nLogLik + nLogPriorM + nLogPriorB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5cf8d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 10000/10000 [04:31<00:00, 36.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 10000\n",
    "\n",
    "#model = nLogLikelyhood_v1()\n",
    "\n",
    "#model = nLogLikelyhood_v2()\n",
    "\n",
    "#model = maxPosterior_v1()\n",
    "\n",
    "learning_rate = 0.02\n",
    "\n",
    "model = variationalBayes_v3()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Training...\"):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #nLogLik = model(x_tensor, y_tensor)\n",
    "    #e = torch.mean(nLogLik)\n",
    "    \n",
    "    nLogLik = model(x_tensor, y_tensor)\n",
    "    nLogLik.backward(retain_graph=True)\n",
    "    #nLogLik.backward()\n",
    "    \n",
    "    #e.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeec4d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(0.0472, requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.f.muM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d410648c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(7.0060, requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.f.muB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47333bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(3.3132, requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sigmaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "747dde3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0252, grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(model.f.sigmaM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d951d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0498, grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(model.f.sigmaB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
