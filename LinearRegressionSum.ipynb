{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Linear Regression is a statistical analysis tool in order to predict the relationship between two variables. Typically, we refer to independent variable as the $x$ variable and the dependent variable as the $y$ variable.\n",
    "\n",
    "These models are linear making them simple which is an advantage because due to their easy to understand nature, it makes them vastly modular for many applications.\n",
    "\n",
    "### Notation\n",
    "Before moving forward, we need to define some notations that will be used throughout this file.\n",
    "\n",
    "- $x$ is independent vector\n",
    "- $y$ is dependent vector\n",
    "- $m$ is the slope\n",
    "- $b$ is the intercept\n",
    "- $\\sigma_m$ is the standard deviation of $m$\n",
    "- $\\sigma_b$ is the standard deviation of $b$\n",
    "- $x_i$ is one element of input vector\n",
    "- $y_i$ is one element of output vector\n",
    "- $\\bar{y}$ is mean of output vector\n",
    "- $\\sigma_n$ is the noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "A linear equation is model using\n",
    "\n",
    "$$ y = mx + b$$\n",
    "\n",
    "where $x$ is the independent variable, $y$ is the dependent variable, and $b$ is the $y$-intercept.\n",
    "\n",
    "Our goal in linear regression is to optimize $m$ and $b$. To be able to optimize any model, we need a way to judge how good the parameters are. For this we can use the coefficient of determination. This will tell us how good the best fit line is from the actual data. The closer $R^2$ is to $1$, the more accurate the best fit line is.\n",
    "\n",
    "$$ R^2 = \\frac{RSS}{TSS} $$\n",
    "\n",
    "where $RSS$ is the sum of squares of residuals and can be found using\n",
    "\n",
    "$$ RSS = \\sum_{i=1}^n (y_i - f(x_i))^2 $$\n",
    "\n",
    "and $TSS$ is the total sum of squares\n",
    "\n",
    "$$ TSS = \\sum_{i=1}^n (y_i - \\bar{y})^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class StraightLine_v1(nn.Module):\n",
    "    '''\n",
    "    This class models a straight line\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = nn.Parameter(torch.tensor(1.0))\n",
    "        self.b = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.m * x + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSquared_v1(nn.Module):\n",
    "    '''\n",
    "    This class implements the R2 loss function\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = StraightLine_v1()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        y_pred = self.f(x)\n",
    "        \n",
    "        RSS = torch.sum((y - y_pred)**2)\n",
    "        TSS = torch.sum((y - torch.mean(y))**2)\n",
    "\n",
    "        return (-1.) * (1 - RSS / TSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data\n",
    "\n",
    "We will be using an advertising dataset with different types of advertising platforms. For our case, we want to know how TV advertising impacts Sales and so our $x$ vector will be TV and our $y$ vector will be Sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Obtains the file and cleans it\n",
    "FILE_PATH = 'data/Advertising.csv' \n",
    "df = pd.read_csv(FILE_PATH).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Obtains our x and y vectors\n",
    "x = df.TV.to_numpy().reshape(-1, 1)\n",
    "y = df.Sales.to_numpy().reshape(-1, 1)\n",
    "\n",
    "x_tensor = Variable(torch.from_numpy(x).type(torch.FloatTensor)) # type: ignore\n",
    "y_tensor = Variable(torch.from_numpy(y).type(torch.FloatTensor)) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "In order to train all of our models, we will use the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, x_tensor, y_tensor, epochs=10000, learning_rate=0.02):\n",
    "    '''\n",
    "    This function trains a model\n",
    "    '''\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training...\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        nLogLik = model(x_tensor, y_tensor)\n",
    "        nLogLik.backward(retain_graph=True)\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 10000/10000 [00:05<00:00, 1757.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor(0.0475, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(7.0326, requires_grad=True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trains the model\n",
    "model_rs = RSquared_v1()\n",
    "train(model_rs, x_tensor, y_tensor)\n",
    "\n",
    "# Prints the results\n",
    "model_rs.f.m, model_rs.f.b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Liklihood Estimation (MLE)\n",
    "\n",
    "Maximum Likelihood Estimation is a method we can use to estimate the parameters of an assumed probability distribution.\n",
    "\n",
    "$$ \\sum_{i=1}^n \\frac{(y_i - mx_i - b)^2}{2\\sigma_n^2} + \\frac{1}{2\\sigma_n^2}$$\n",
    "\n",
    "Before we can do bayesian inference, this is equivalent to maximum a posteriori (MAP) estimation with uniform prior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nLogLikelyhood_v1(nn.Module):\n",
    "    '''\n",
    "    This class models the log likelyhood of a straight line\n",
    "    which is used to assess the quality of the fit\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = StraightLine_v1()\n",
    "        self.sigmaN = nn.Parameter(torch.tensor(1.0))\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        pred = self.f(x)\n",
    "        return 0.5 * (torch.log(self.sigmaN ** 2) + 0.5 * ((y - pred) ** 2) / (self.sigmaN ** 2)).sum()\n",
    "        # The line above is equivalent to the following:\n",
    "        # return (-1.) * Normal(pred, self.sigmaN).log_prob(y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 10000/10000 [00:07<00:00, 1402.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor(0.0475, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(7.0326, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(2.2635, requires_grad=True))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mle = nLogLikelyhood_v1()\n",
    "train(model_mle, x_tensor, y_tensor)\n",
    "\n",
    "model_mle.f.m, model_mle.f.b, model_mle.sigmaN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum a Posteriori Estimation (MAP)\n",
    "\n",
    "Maximum a Posteriori Estimation adds on to the maximum likelihood estimation and can be seen as a regularization of maximum likelihood estimation.\n",
    "\n",
    "$$-\\log{P(y \\mid X, w) * P(m) * P(b)}$$\n",
    "\n",
    "To find both $P(m)$ and $P(b)$ we need to use these equation...\n",
    "\n",
    "$$P(m) = \\log\\sigma_m + \\frac{m^2}{2\\sigma_m^2}$$\n",
    "\n",
    "$$ \\& $$\n",
    "\n",
    "$$P(b) = \\log\\sigma_b + \\frac{b^2}{2\\sigma_b^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "\n",
    "class maxPosterior_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = StraightLine_v1()\n",
    "        self.sigmaN = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        pred = self.f(x)\n",
    "        nLogLik = (-1.) * Normal(pred, self.sigmaN).log_prob(y).sum()\n",
    "        \n",
    "        nLogPriorM = (-1.) * Normal(0, 1).log_prob(self.f.m)\n",
    "        nLogPriorB = (-1.) * Normal(0, 10).log_prob(self.f.b)\n",
    "        \n",
    "        return nLogLik + nLogPriorM + nLogPriorB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 10000/10000 [00:15<00:00, 640.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor(0.0476, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(7.0218, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(2.8215, requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_map = maxPosterior_v1()\n",
    "train(model_map, x_tensor, y_tensor)\n",
    "\n",
    "model_map.f.m, model_map.f.b, model_map.sigmaN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Straight Line\n",
    "\n",
    "So far we have obtained only one slope and one intercept but we want a distribution for both the slope and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StraightLine_v2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.muM = nn.Parameter(torch.tensor(1.0))\n",
    "        self.sigmaM = nn.Parameter(torch.tensor(1.0))\n",
    "        self.muB = nn.Parameter(torch.tensor(1.0))\n",
    "        self.sigmaB = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "        self.samples = 100\n",
    "        \n",
    "        self.m = self.muM + torch.exp(self.sigmaM) * Normal(0, 1).sample(torch.Size([1, self.samples]))\n",
    "        self.b = self.muB + torch.exp(self.sigmaB) * Normal(0, 1).sample(torch.Size([1, self.samples]))\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        self.m = self.muM + torch.exp(self.sigmaM) * Normal(0, 1).sample(torch.Size([1, self.samples]))\n",
    "        self.b = self.muB + torch.exp(self.sigmaB) * Normal(0, 1).sample(torch.Size([1, self.samples]))\n",
    "\n",
    "        return torch.matmul(x, self.m) + self.b.repeat(x.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence\n",
    "\n",
    "This allows us to measure how one probability distributition can be different from another.\n",
    "\n",
    "We want to emulate...\n",
    "$$ -\\int q(w) \\log\\frac{p(y \\mid x, w)p(w)}{q(w)} dw $$\n",
    "\n",
    "Which is approximately equal to...\n",
    "$$ \\frac{1}{M}\\sum_{i=1}^M\\log P(y \\mid x, w^{(i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class variationalBayes_v1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.f = StraightLine_v2()\n",
    "        self.sigmaN = nn.Parameter(torch.tensor(1.0))\n",
    "        self.prior_M_std = torch.tensor([1.])\n",
    "        self.prior_B_std = torch.tensor([10.])\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        pred = self.f(x)\n",
    "        \n",
    "        y_truth = y.reshape(y.shape[0], -1).repeat(1, self.f.samples)\n",
    "        nLogLik = (-1.) * Normal(pred, self.sigmaN).log_prob(y_truth).sum() / self.f.samples\n",
    "\n",
    "        nLogPriorM = torch.log(self.prior_M_std) + 0.5 * (self.f.muM / self.prior_M_std) ** 2 + 0.5 * (self.f.sigmaM / self.prior_M_std) ** 2\n",
    "        nLogPriorB = torch.log(self.prior_B_std) + 0.5 * (self.f.muB / self.prior_B_std) ** 2 + 0.5 * (self.f.sigmaB / self.prior_B_std) ** 2\n",
    "        \n",
    "        LogVarPostM = (-1.) * torch.log(self.f.sigmaM) \n",
    "        LogVarPostB = (-1.) * torch.log(self.f.sigmaB)\n",
    "        \n",
    "        return LogVarPostM + LogVarPostB + nLogLik + nLogPriorM + nLogPriorB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 10000/10000 [00:56<00:00, 177.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor(0.0495, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(7.0114, requires_grad=True),\n",
       " tensor(0.0248, grad_fn=<ExpBackward0>),\n",
       " tensor(1.0513, grad_fn=<ExpBackward0>),\n",
       " Parameter containing:\n",
       " tensor(3.3328, requires_grad=True))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = variationalBayes_v1()\n",
    "train(model, x_tensor, y_tensor)\n",
    "\n",
    "model.f.muM, model.f.muB, torch.exp(model.f.sigmaM), torch.exp(model.f.sigmaB), model.sigmaN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
