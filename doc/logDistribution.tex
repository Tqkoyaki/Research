%\documentclass{article}
\documentclass[letter,11pt]{article}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{amsmath}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{Note on variational infernece}

%\maketitle
\begin{document}
\maketitle

In the question of Bayesian learning of a linear relation between the input ${\bf x}$ and output $y$, we assume the Gaussian likelihood,
\begin{equation}
	p(y|{\bf x}, {\bf w}, \tau)=\mathcal N(y|{\bf w}\cdot{\bf x},1/\tau)\:,
\end{equation} where the variance being $1/\tau$. The prior over the parameters is
\begin{equation}
	p({\bf w}) = \mathcal N({\bf w}|0, (\tau\alpha)^{-1}I)\:,
\end{equation} the prior over the noise parameter, 
\begin{equation}
	p(\tau)=G(\tau|a_0,b_0)\:,
\end{equation} and the prior over the sacle,
\begin{equation}
	p(\alpha)=G(\alpha|c_0,d_0)
\end{equation}

In the one-level variational Bayesian (VB) inference, one often needs to compute the following expectation, 
\begin{equation}
\begin{split}
	-\int dx \mathcal N(x|\mu_1,\sigma_1^2)\log \mathcal N(x|\mu_2,\sigma_2^2) 
	&= \mathbb E_{x\sim\mathcal N(\mu_1,\sigma_1^2)}
	\big[
	\frac{(x-\mu_1+\mu_1-\mu_2)^2}{2\sigma_2^2}
	\big] 
	+\frac{1}{2}\log(2\pi\sigma_2^2) \\
	&= %\frac{1}{2}\big[
	\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2} + \frac{1}{2}\log(2\pi\sigma_2^2)\:,
	%\big]
\end{split}
\end{equation} which appears in the KL divergence. In the two-level VB, then one needs to compute the following integral,
\begin{equation}
\begin{split}
	&-\int dxd\tau G(\tau|a,b)\mathcal N(x|\mu_1,\sigma_1^2)\log \mathcal N(x|\mu_2,\tau^{-1}\sigma_2^2) \\
	&= \mathbb E_{\tau\sim G(a,b)}
	\big[
	\tau\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2} - \frac{1}{2}\log\tau
	%\frac{(x-\mu_1+\mu_1-\mu_2)^2}{2\sigma_2^2}
	\big] + \frac{1}{2}\log(2\pi\sigma_2^2)\\
	%+\frac{1}{2}\log(2\pi\sigma_2^2) \\
	&= \frac{1}{2}\bigg[
	\frac{a}{b}\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{\sigma_2^2} 
	-\psi(a) +\log b
	+ \log(2\pi\sigma_2^2)
	\bigg]\:.
\end{split}
\end{equation} As for the Gamma distribution,
\begin{equation}
	\int d\tau G(\tau|a,b)\log G(\tau|a,b) = -\log\Gamma(a) + (a-1)\psi(a) + \log b - a
\end{equation}
%The next code will be directly imported from a file

In the variational inference, we have assumed the form of the variational distribution over the weight parameters ${\bf w}$ and scale parameter $\tau$,
\begin{equation}
	Q({\bf w},\tau) = \mathcal N({\bf w}|{\bf w}_{BV}, \tau^{-1}{\bf V})
	G(\tau|a, b)\:,
\end{equation} with the variational parameters ${\bf w}_{VB}$, the diagonal covariance matrix ${\bf V}$ along with the parmaeters $a$ and $b$ in the Gamma distribution. The distribution for $\alpha$ is given by,
\begin{equation}
	Q(\alpha) = G(\alpha|c, d)\:, 
\end{equation} with variational parameters $c$ and $d$. It can be shown that
\begin{equation}
\begin{split}
	\mathbb E_Q[\log Q({\bf w},\tau)] = &-\frac{D}{2}\big[
	\frac{a}{b} - \psi(a) + \log b + \log |{\bf V}|
	\big]\\
	&-\log\Gamma(a) + (a-1)\psi(a) + \log b - a\:,
\end{split}
\end{equation} and
\begin{equation}
	\mathbb E_Q[Q(\alpha|c,d)]= -\log\Gamma(c) + (c-1)\psi(c) + \log d - c
\end{equation}

%\lstinputlisting[language=Octave]{BitXorMatrix.m}

\begin{lstlisting}[language=Python, caption=Python example]
import numpy as np
    
def incmatrix(genl1,genl2):
    m = len(genl1)
    n = len(genl2)
    M = None #to become the incidence matrix
    VT = np.zeros((n*m,1), int)  #dummy variable

\end{lstlisting}

\end{document}